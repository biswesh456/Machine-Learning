# -*- coding: utf-8 -*-
"""Lenet_Implementation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NhgC7P0xjXeBgfe-YAeguCy6CBVt_AwC
"""

!pip3 install torch

import torch
import numpy as np
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms
import matplotlib.pyplot as plt

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

torch.manual_seed(1)

transform = transforms.Compose([transforms.ToTensor(),
                               transforms.Normalize([0.5], [0.5])
                               ])
training_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
training_loader = torch.utils.data.DataLoader(dataset=training_dataset, batch_size=100, shuffle=True) 
validation_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)
validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=100, shuffle=False)

class LeNet(nn.Module):
  def __init__(self):
    super().__init__()
    self.conv1 = nn.Conv2d(1, 20, 5, stride=1)
    self.conv2 = nn.Conv2d(20, 50, 5, stride=1)
    self.fc1 = nn.Linear(4*4*50, 500)
    self.dropout1 = nn.Dropout(0.5)
    self.fc2 = nn.Linear(500, 10)
    
  def forward(self, x):
    x = F.relu(self.conv1(x))
    x = F.max_pool2d(x, 2, 2)
    x = F.relu(self.conv2(x))
    x = F.max_pool2d(x, 2, 2)
    x = x.view(-1, 4*4*50)
    x = F.relu(self.fc1(x))
    x = self.dropout1(x)
    x = self.fc2(x)
#   since we are using crossentropy loss, it already has softmax in it, so raw
#   output should be final without any activation function   
    return x

model = LeNet().to(device)
model

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

epochs = 15
history=[]
val_history=[]
for e in range(epochs):
  current_loss = 0.0
  running_correct = 0.0
  validation_loss = 0.0
  validation_acc = 0.0
  for inputs, labels in training_loader:
    inputs = inputs.to(device)
    labels = labels.to(device)
    
#     inputs = inputs.view(inputs.shape[0], -1)   this is required is passing 1 dimensional input
    outputs = model(inputs)
    loss = criterion(outputs, labels)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    _, preds = torch.max(outputs, 1)
    
    current_loss += loss.item()
    running_correct += torch.sum(preds == labels.data)

  else:
    with torch.no_grad():
      for val_inputs, val_labels in validation_loader:
        val_inputs = val_inputs.to(device)
        val_labels = val_labels.to(device)
        
#         val_inputs = val_inputs.view(val_inputs.shape[0], -1)
        val_output = model(val_inputs)
        val_loss = criterion(val_output, val_labels)
        
        _, val_preds = torch.max(val_output, 1)
        validation_loss += val_loss.item()
        validation_acc += torch.sum(val_preds == val_labels.data)
           
      epoch_loss = current_loss/(len(training_loader))
      epoch_acc = running_correct.float()/len(training_loader)
      validation_loss = validation_loss/(len(validation_loader))
      validation_acc = validation_acc.float()/len(validation_loader)
      history.append(epoch_loss)
      print('training loss: {:.4f}, training accuracy: {:.4f}, validation loss: {:.4f}, validation accuracy: {:.4f}'.format(epoch_loss, epoch_acc.item(), validation_loss, validation_acc.item()))

plt.plot(history, label="training loss")
plt.plot(val_history, label="validation loss")
plt.legend()

# we need a smaller LR for smoother curve. 
# Look into validation in the video

